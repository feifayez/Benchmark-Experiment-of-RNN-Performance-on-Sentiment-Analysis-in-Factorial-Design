{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The original study intends to test how the type of pretrained word vector and vocabulary size impact a basic RNN's performance on sentiment analysis. However, the analyst had difficulty to attain fastTex in .txt format and wasn't able to find a solution before the deadline. Therefore, the study was modified to test how vocabulary size and dimension of embedding impact a basic RNN's performance on sentiment analysis. Since the embedding dimension decides the number of inputs of the RNN, this study also exams the impact of RNN design on sentiment analysis result.** \n",
    "\n",
    "\n",
    "**As mentioned, the study was conducted in a 2x2 factorial design. Factor 1: vocabulary size; Factor 2: dimension of embedding. To satisfy the design, four (4) pretrained word vectors were utilized: gloVe.6b.50d (50 dimensions, 400K vocabulary size), gloVe.6b.100d (100 dimensions, 400K vocabulary size), gloVe.Twitter.50d (50 dimensions, 1.2M vocabulary size), gloVe.Twitter.100d (100 dimensions, 1.2M vocabulary size).** \n",
    "\n",
    "\n",
    "**The initial analysis of movie reviews data shows that the reviews vary from 22 to 1052 words. It was decided to use the first 20 and last 20 words of each review as the word sequences for analysis. The 500 negative reviews and 500 positive reviews were converted in to a list of 1000 lists of 40 words in each list.**\n",
    "\n",
    "\n",
    "**The 10000 most frequently used words from each of the four (4) embeddings were utilized to construct the embedding to map the preprocessed review data (list of 10000 lists of 40 words) to a numpy array in the shape of (1000, 40, original embedding dimension). This numpy array was used inputs of the RNN. Another numpy array of 500 0s (Thumbs down) and 500 1s (Thumbs up) was constructed as dependent variable. Simple train/test split was used as cross-validation method for this study. 80% of the data was randomly chose as train set while the rest was used as test set.**\n",
    "\n",
    "\n",
    "**A total of (4) tests were run under the 2x2 factor design. Model's accuracy on train set and accuracy on test set were used as performance index. The results show that the basic RNN is prone to overfitting. When the number of inputs was held constant, the RNN designed with embedding compressed from a pre-trained word vector with larger vocabulary provides better predictive accuracy than the one designed with embedding from a smaller vocabulary vector. The result also shows that the RNN model with more inputs, in other words, utilizing embedding of higher dimensions, achieved better predictive accuracy on both train and test set than the one with less inputs (lower dimension).** \n",
    "\n",
    "\n",
    "**For this study, we preprocessed the review data to include only the first 20 and last 20 words of each review document. We also utilized compressed embeddings instead of the full pre-trained word vectors to build the RNN. We also noticed overfitting with the basic RNN structure. Therefore, as the next step, the management should have data scientists to (1) test RNN performance with different word sequences; (2) test RNN performances with full pre-trained vectors of different vocabulary size; (3) utilize more sophisticated RNN structure including LMST/GRU cell, dropout layer, additional fully-connected layer, etc; (4) consider a multiclass classification where the most critical reviews, either the most positive or the most negative ones, can be identified** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following codes are borrowed and slightly modified from Miller (2018) run-jump-start-rnn-sentiment-v002.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed packages for this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup to make output stable across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No stopword removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVIE REVIEWS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for working with movie reviews data. Source: Miller, T. W. (2016). Web and Network Data Science.Upper Saddle River, N.J.: Pearson Education. ISBN-13: 978-0-13-388644-3\n",
    "This original study used a simple bag-of-words approach to sentiment analysis, along with pre-defined lists of negative and positive words. Code available at:  https://github.com/mtpa/wnds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility function to get file names within a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define list of codes to be dropped from document carriage-returns, line-feeds, tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "codelist = ['\\r', '\\n', '\\t']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not remove stopwords in this research as they are important to keep sentence intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a number of words along with contractions and other word strings to the usual English stopwords to be dropped from a document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "stoplist = nltk.corpus.stopwords.words('english') + more_stop_words + some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define text parsing function for creating text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definte read_data function to store move reviews data in a list of lists where each list represents a document and document is a list of words. We then break the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = text_parse(data)\n",
    "        data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather data for 500 negative movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather data for 500 positive movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the maximum length and minimum length of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that reviews vary from 22 to 1052 words. We will use the first 20 and last 20 words of each review as our word sequences for analysis. Convert positive/negative documents into numpy array of 1000 lists (500 positive reviews & 500 negative reviews) with 40 words in each list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRETRAINED WORD VECTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test how vocabulary size and embedding dimension may influence the performance of sentimental analysis with RNN, we will utilize four (4) embeddings: gloVe.6b.50d (50 dimensions, 400K vocabulary size), gloVe.6b.100d (100 dimensions, 400K vocabulary size), gloVe.Twitter.50d (50 dimensions, 1.2M vocabulary size), gloVe.Twitter.100d (100 dimensions, 1.2M vocabulary size). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup utility function for loading embeddings follows methods as described in (https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the first embedding: gloVe.6b.50d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "embeddings_directory = 'embeddings/gloVe.6B' # GloVe.840B.300d (300 dimensions and 2.2 M size/fastText(en) (300 dimensions and 2.5M)\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index_6B5, index_to_embedding_6B5 = load_embedding_from_disks(embeddings_filename, \n",
    "                                                                      with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the second embedding: gloVe.6b.100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.100d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "embeddings_directory = 'embeddings/gloVe.6B' \n",
    "filename = 'glove.6B.100d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index_6B10, index_to_embedding_6B10 = load_embedding_from_disks(embeddings_filename, \n",
    "                                                                        with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the third embedding: gloVe.Twitter.50d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.twitter.27B\\glove.twitter.27B.50d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "embeddings_directory = 'embeddings/gloVe.twitter.27B' \n",
    "filename = 'glove.twitter.27B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index_T5, index_to_embedding_T5 = load_embedding_from_disks(embeddings_filename, \n",
    "                                                                    with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fourth embedding: gloVe.Twitter.100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.twitter.27B\\glove.twitter.27B.100d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "embeddings_directory = 'embeddings/gloVe.twitter.27B' \n",
    "filename = 'glove.twitter.27B.100d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index_T10, index_to_embedding_T10 = load_embedding_from_disks(embeddings_filename, \n",
    "                                                                      with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK TEST WITH 2 X 2 FACTORIAL DESIGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up numpy array to host the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Accuracy=[]\n",
    "Test_Accuracy=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define vocabulary size for the language model. To reduce the size of the vocabulary to the 10000 most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVOCABSIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_factory():\n",
    "    return EVOCABSIZE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of four (4) tests are run in the 2x2 factorial design: Test 1: original vocabulary size = 400K, embedding dimension = 50; Test 2: original vocabulary size = 1.2M, embedding dimension = 50; Test 3: original vocabulary size = 400K, embedding dimension = 100; Test 4: original vocabulary size = 1.2M, embedding dimension = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct RNN for Test 1 and Test 2 with number of neurons set to be 20 and learning rate set to be 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 40  # number of words per document \n",
    "n_inputs = 50  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct Test 1 and Test 2: in each loop, we first define the limited dictionary based on desired vocabulary size of 10000 and the word vector, then create list of lists of lists for embeddings which is converted to a numpy array to be used in a basic RNN for sentiment analysis. For each test, we run 50 epoches. The accuracy on train set and test set are recorded for comparison analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = [word_to_index_6B5, word_to_index_T5]\n",
    "index2embd = [index_to_embedding_6B5, index_to_embedding_T5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_to_index, index_to_embedding in zip(word2index, index2embd):\n",
    "    limited_word_to_index = defaultdict(default_factory, {k: v for k, v in word_to_index.items() \n",
    "                                                          if v < EVOCABSIZE})\n",
    "    limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE, :]\n",
    "    limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "                                           index_to_embedding[index_to_embedding.shape[0] - 1, :].reshape(1,\n",
    "                                           index_to_embedding.shape[1]), axis=0)\n",
    "    del index_to_embedding\n",
    "    embeddings = []\n",
    "    for doc in documents:\n",
    "        embedding = []\n",
    "        for word in doc:\n",
    "            embedding.append(limited_index_to_embedding[limited_word_to_index[word]])\n",
    "        embeddings.append(embedding)\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                                     np.ones((500), dtype = np.int32)), axis = 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings_array, \n",
    "                                                        thumbs_down_up, test_size=0.20, \n",
    "                                                        random_state = RANDOM_SEED)\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(y_train.shape[0] // batch_size):\n",
    "                X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "                y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "                acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        Train_Accuracy.append(acc_train)\n",
    "        Test_Accuracy.append(acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct RNN for Test 3 and Test 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 40  # number of words per document \n",
    "n_inputs = 100  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct Test 3 and Test 4: in each loop, we first define the limited dictionary based on desired vocabulary size of 10000 and the word vector, then create list of lists of lists for embeddings which is converted to a numpy array to be used in a basic RNN for sentiment analysis. For each test, we run 50 epoches. The accuracy on train set and test set are recorded for comparison analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = [word_to_index_6B10, word_to_index_T10]\n",
    "index2embd = [index_to_embedding_6B10, index_to_embedding_T10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_to_index, index_to_embedding in zip(word2index, index2embd):\n",
    "    limited_word_to_index = defaultdict(default_factory, {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "    limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE, :]\n",
    "    limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "                                           index_to_embedding[index_to_embedding.shape[0] - 1, :].reshape(1,\n",
    "                                           index_to_embedding.shape[1]), axis=0)\n",
    "    del index_to_embedding\n",
    "    embeddings = []\n",
    "    for doc in documents:\n",
    "        embedding = []\n",
    "        for word in doc:\n",
    "            embedding.append(limited_index_to_embedding[limited_word_to_index[word]])\n",
    "        embeddings.append(embedding)\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                                     np.ones((500), dtype = np.int32)), axis = 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings_array, \n",
    "                                                        thumbs_down_up, test_size=0.20, \n",
    "                                                        random_state = RANDOM_SEED)\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(y_train.shape[0] // batch_size):\n",
    "                X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "                y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "                acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        Train_Accuracy.append(acc_train)\n",
    "        Test_Accuracy.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82, 0.84, 0.86, 0.86]\n"
     ]
    }
   ],
   "source": [
    "print(Train_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62, 0.69, 0.67, 0.765]\n"
     ]
    }
   ],
   "source": [
    "print(Test_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [50, 50, 100, 100]\n",
    "vocabularysize = ['400K', '1.2M', '400K', '1.2M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(OrderedDict([\n",
    "                        ('Dimensions', dimensions),\n",
    "                        ('Vocabulary_Size', vocabularysize),\n",
    "                        ('Train_Accuracy', Train_Accuracy),\n",
    "                        ('Test_Accuracy', Test_Accuracy),\n",
    "                        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dimensions Vocabulary_Size  Train_Accuracy  Test_Accuracy\n",
      "0          50            400K            0.82          0.620\n",
      "1          50            1.2M            0.84          0.690\n",
      "2         100            400K            0.86          0.670\n",
      "3         100            1.2M            0.86          0.765\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first notice that the train accuracy was higher than test accuracy across all four (4) tests. This indicates that the current RNN structure is overfitting. We should consider using techniques such as dropout, batch normalization, LMST or GRU cell, etc to reduce the probability of overfitting. \n",
    "\n",
    "When n_inputs = 50, the RNN which utilized embedding compressed from a pre-trained word vectors of larger vocabulary outperformed the one whose embedding was drawn from a smaller vocabulary word vector on both train set and test set. When n_inputs = 100, the RNN utilized embedding from a larger vocabulary outperformed the one with embedding from smaller vocabulary on test set. Based on the results of the four (4) tests, it seems that the size of the vocabulary of pre-trained word vectors is positively associated with the predictive accuracy of RNN on sentiment analysis. \n",
    "\n",
    "When the vocabulary size was held constant, the RNN built with embeddings of higher dimensions, in other words, the RNN with more inputs, consistently outperforms the one with lower dimensions (less inputs). This was evident by comparing test 1 and test 3, test 2 and test 4. \n",
    "\n",
    "Therefore, the management is recommended to use a RNN system dedicately designed with LMST/GRU cell and dropout layer, instead of a basic structure, to avoide overfitting. The management should also consider utilizing embedding of higher dimensions as it will improve the model's performance. Depending on the field, the management may find none of the pre-trained word vectors useful as they don't provide the unique details required by the specific industry. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
